{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing public libraries\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import epynet\n",
    "import yaml\n",
    "import torch\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom libraries\n",
    "from utils.epanet_loader import get_nx_graph\n",
    "from utils.visualisation import visualise\n",
    "from utils.epanet_simulator import epanetSimulator\n",
    "from utils.data_loader import battledimLoader, dataCleaner, dataGenerator, embedSignalOnGraph, rescaleSignal, predictionTaskDataSplitter\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from modules.torch_gnn import ChebNet\n",
    "import utils.user_interface as ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: \t 'import_model.py' \n",
      "Using device: \t cuda\n",
      "\n",
      "Configurations for session: \n",
      "----------------------\n",
      "WDN: l-town\n",
      "GNN: chebnet\n",
      "Weights: pipe_length\n",
      "Visualise: yes\n",
      "Visualise What: sensor_location\n",
      "Scaling: minmax\n",
      "Tag: predictor\n",
      "Epochs: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nRunning: \\t 'import_model.py' \")\n",
    "print(\"Using device: \\t {}\".format(device))\n",
    "\n",
    "# Hardcoded configurations\n",
    "wdn = 'l-town'  # Choice of water-distribution network\n",
    "gnn = 'chebnet'  # Choice of GNN model\n",
    "# weights = 'unweighted'  # Edge weights type\n",
    "weights = 'pipe_length'  # Edge weights type\n",
    "visualise = 'yes'  # Visualise the graph\n",
    "visualiseWhat = 'sensor_location'  # What to visualise\n",
    "scaling = 'minmax'  # Rescaling method\n",
    "tag = 'predictor'  # Descriptive tag\n",
    "epochs = 100  # Number of training epochs\n",
    "timesteps = 3  # Number of timesteps in the signal\n",
    "\n",
    "self_loop = True \n",
    "\n",
    "print(\"\\nConfigurations for session: \\n\" + 22*\"-\")\n",
    "print(\"WDN: {}\\nGNN: {}\\nWeights: {}\\nVisualise: {}\\nVisualise What: {}\\nScaling: {}\\nTag: {}\\nEpochs: {}\\n\".format(\n",
    "    wdn, gnn, weights, visualise, visualiseWhat, scaling, tag, epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment paths...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the filepaths for the execution\n",
    "print('Setting environment paths...\\n')\n",
    "\n",
    "path_to_data   = './data/' + wdn + '-data/'       # Datasets are stored here\n",
    "path_to_wdn    = './data/' + wdn.upper() + '.inp' # EPANET input file\n",
    "path_to_logs   = './studies/logs/'                     # Log directory\n",
    "path_to_figs   = './studies/figs/'                     # Figure directory\n",
    "path_to_models = './studies/models/'                   # Saved models directory               \n",
    "\n",
    "execution_no   = 1                                                                                      # Initialise execution ID number\n",
    "execution_id   = wdn + '-' + gnn + '-' + tag + '-' + weights + '-' + scaling + '-' + '{}'.format('self_loop-' if self_loop else '')+ '-' + str(timesteps)  # Initialise execution ID name\n",
    "logs           = [log for log in os.listdir(path_to_logs) if log.endswith('.csv')]                      # Load all logs in directory to list\n",
    "\n",
    "while execution_id + str(execution_no) + '.csv' in logs:    # For every matching file name in the directory\n",
    "    execution_no += 1                                       # Increment the execution id number\n",
    "\n",
    "execution_id   = execution_id + str(execution_no)           # Update the execution ID\n",
    "model_path     = path_to_models + execution_id + '.pt'      # Generate complete model path w/ filename\n",
    "log_path       = path_to_logs   + execution_id + '.csv'     # Generate complete log path w/ filename\n",
    "\n",
    "# If we have already trained a similar model we may wish to load its weights\n",
    "# So, we must also generate a path to that model's state dictionary\n",
    "if execution_no > 1:\n",
    "    last_id         = wdn + '-' + gnn + '-' + tag + '-' # Initialise previous version execution ID name\n",
    "    last_id         = last_id + str(execution_no - 1)                  # Execution ID number is the current number - 1  \n",
    "    last_model_path = path_to_models + last_id + '.pt'                 # Generate complete path to the previously trained model\n",
    "    last_log_path   = path_to_logs   + last_id + '.csv'                # Generate complete path to the previous \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing EPANET file and converting to graph...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Importing EPANET file and converting to graph...\\n')\n",
    "\n",
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()\n",
    "\n",
    "# Convert the file using a custom function, based on:\n",
    "# https://github.com/BME-SmartLab/GraphConvWat \n",
    "G , pos , head = get_nx_graph(wdn, weight_mode=weights, get_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset configuration...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Importing dataset configuration...\\n')\n",
    "\n",
    "# Open the dataset configuration file\n",
    "with open(path_to_data + 'dataset_configuration.yaml') as file:\n",
    "    \n",
    "    # Load the configuration to a dictionary\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader) \n",
    "\n",
    "# Generate a list of integers, indicating the number of the node\n",
    "# at which a  pressure sensor is present\n",
    "sensors = [int(string.replace(\"n\", \"\")) for string in config['pressure_sensors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_node in sensors:\n",
    "    G.add_edge(u_of_edge=sensor_node,\n",
    "                v_of_edge=sensor_node,\n",
    "                weight=1.,name='SELF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EPANET simulation to generate nominal pressure data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Running EPANET simulation to generate nominal pressure data...\\n')\n",
    "nominal_wdn_model = epanetSimulator(path_to_wdn, path_to_data)\n",
    "nominal_wdn_model.simulate()\n",
    "nominal_pressure = nominal_wdn_model.get_simulated_pressure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_pressure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing nominal pressure data for training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Pre-processing nominal pressure data for training...\\n')\n",
    "\n",
    "# Populate feature vector x and label vector y from the nominal pressures\n",
    "# Also retrieve the scale and bias of the scaling transformation\n",
    "# This is so we can inverse transform the predicted values to calculate\n",
    "# relative reconstruction errors\n",
    "x,y,scale,bias = dataCleaner(pressure_df    = nominal_pressure, # Pass the nodal pressures\n",
    "                             observed_nodes = sensors,          # Indicate which nodes have sensors\n",
    "                             rescale        = scaling,          # Perform scaling on the timeseries data\n",
    "                             task           = 'prediction',\n",
    "                             mode           = 'n_timesteps',\n",
    "                             n_timesteps    = 3)     \n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, \n",
    "                                              test_size    = 0.2,\n",
    "                                              random_state = 1,\n",
    "                                              shuffle      = False)\n",
    "\n",
    "# # Now, we change the data so it matches a prediction task, where t, t-1, ..., t-n in x refer to t+1 in y\n",
    "# x_p, y_p = predictionTaskDataSplitter(x, y, timesteps)                  # <------------------------------------------------- Note this is where we are using the timesteps\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# x_trn, x_val, y_trn, y_val = train_test_split(x_p, y_p,\n",
    "#                                                 test_size    = 0.2,\n",
    "#                                                 random_state = 1,\n",
    "#                                                 shuffle      = False)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 782, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611, 782, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1833793"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trn[0,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1842804"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn[0,3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611, 782, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training session and creating model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Setting up training session and creating model...\\n')\n",
    "\n",
    "# ----------------\n",
    "# Hyper-parameters\n",
    "# ----------------\n",
    "batch_size    = 40\n",
    "learning_rate = 3e-4\n",
    "decay         = 6e-6\n",
    "shuffle       = False\n",
    "epochs        = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30806\\anaconda3\\envs\\Msc\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate training and test set data generators\n",
    "trn_gnrtr = dataGenerator(G, x_trn, y_trn, batch_size, shuffle)\n",
    "val_gnrtr = dataGenerator(G, x_val, y_val, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Chebysev Network GNN model\n",
    "model     = ChebNet(name           = 'ChebNet',\n",
    "                    data_generator = trn_gnrtr,\n",
    "                    device         = device,\n",
    "                    in_channels    = np.shape(x_trn)[-1],\n",
    "                    out_channels   = np.shape(y_trn)[-1],\n",
    "                    data_scale     = scale,\n",
    "                    data_bias      = bias).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer\n",
    "optimizer = torch.optim.Adam([dict(params=model.conv1.parameters(), weight_decay=decay),\n",
    "                            dict(params=model.conv2.parameters(), weight_decay=decay),\n",
    "                            dict(params=model.conv3.parameters(), weight_decay=decay),\n",
    "                            dict(params=model.conv4.parameters(), weight_decay=0)],\n",
    "                            lr  = learning_rate,\n",
    "                            eps = 1e-7)\n",
    "\n",
    "# Configure an early stopping callback\n",
    "estop    = EarlyStopping(min_delta=.00001, patience=epochs)       # By setting patience as # of epochs we make sure early stopping is never initiated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| Training session |\n",
      "| Timesteps: 3    |\n",
      "+------------------+\n",
      "\n",
      "Training starting...\n",
      "\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      "  1     0.022993      0.002237      0.038143      0.044432      0.037866    40.04  sec\n",
      "  2     0.001497      0.001073      0.020058      0.020791      0.020025    39.08  sec\n",
      "  3     0.000876      0.000651      0.014846      0.012750      0.014938    39.38  sec\n",
      "  4     0.000511      0.000370      0.013743      0.013412      0.013758    39.48  sec\n",
      "  5     0.000253      0.000177      0.009681      0.010130      0.009661    39.63  sec\n",
      "  6     0.000129      0.000086      0.006529      0.006944      0.006511    39.72  sec\n",
      "  7     0.000082      0.000083      0.007313      0.007980      0.007283    39.62  sec\n",
      "  8     0.000072      0.000058      0.006091      0.006429      0.006076    39.61  sec\n",
      "  9     0.000076      0.000066      0.006742      0.008259      0.006675    39.65  sec\n",
      " 10     0.000067      0.000056      0.006163      0.006569      0.006145    39.64  sec\n",
      " 11     0.000065      0.000055      0.006162      0.006636      0.006141    39.80  sec\n",
      " 12     0.000066      0.000038      0.004790      0.005369      0.004764    39.65  sec\n",
      " 13     0.000054      0.000042      0.005379      0.005534      0.005372    39.76  sec\n",
      " 14     0.000052      0.000063      0.007209      0.008481      0.007153    39.75  sec\n",
      " 15     0.000055      0.000042      0.005290      0.005539      0.005279    39.66  sec\n",
      " 16     0.000066      0.000058      0.006623      0.007656      0.006577    39.69  sec\n",
      " 17     0.000079      0.000041      0.005216      0.006155      0.005175    39.88  sec\n",
      " 18     0.000056      0.000034      0.004786      0.004752      0.004788    39.64  sec\n",
      " 19     0.000062      0.000038      0.005327      0.005549      0.005317    39.80  sec\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      " 20     0.000053      0.000042      0.005503      0.006228      0.005471    40.28  sec\n",
      " 21     0.000047      0.000051      0.006567      0.007001      0.006548    39.99  sec\n",
      " 22     0.000069      0.000090      0.009093      0.010791      0.009018    39.66  sec\n",
      " 23     0.000095      0.000040      0.005503      0.005804      0.005490    39.76  sec\n",
      " 24     0.000071      0.000057      0.006979      0.008097      0.006930    39.60  sec\n",
      " 25     0.000042      0.000057      0.007023      0.007990      0.006981    39.76  sec\n",
      " 26     0.000044      0.000038      0.005277      0.006007      0.005244    39.78  sec\n",
      " 27     0.000043      0.000035      0.005240      0.005478      0.005230    39.79  sec\n",
      " 28     0.000037      0.000030      0.004741      0.004848      0.004736    39.71  sec\n",
      " 29     0.000045      0.000078      0.008559      0.009623      0.008512    39.90  sec\n",
      " 30     0.000059      0.000045      0.006098      0.006864      0.006064    39.80  sec\n",
      " 31     0.000058      0.000051      0.006567      0.007033      0.006547    39.77  sec\n",
      " 32     0.000043      0.000046      0.006102      0.007138      0.006056    39.81  sec\n",
      " 33     0.000077      0.000054      0.006825      0.007582      0.006792    39.64  sec\n",
      " 34     0.000051      0.000054      0.006820      0.007515      0.006790    39.78  sec\n",
      " 35     0.000056      0.000035      0.005124      0.005807      0.005094    39.80  sec\n",
      " 36     0.000064      0.000029      0.004425      0.005174      0.004392    39.72  sec\n",
      " 37     0.000037      0.000031      0.004664      0.005470      0.004628    39.82  sec\n",
      " 38     0.000048      0.000028      0.004305      0.004953      0.004277    39.84  sec\n",
      " 39     0.000043      0.000039      0.005690      0.006151      0.005670    39.87  sec\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      " 40     0.000067      0.000084      0.009065      0.010192      0.009015    39.82  sec\n",
      " 41     0.000091      0.000093      0.009508      0.010081      0.009483    39.85  sec\n",
      " 42     0.000085      0.000037      0.005292      0.006014      0.005260    39.80  sec\n",
      " 43     0.000106      0.000101      0.009621      0.010301      0.009591    39.85  sec\n",
      " 44     0.000060      0.000052      0.006795      0.007744      0.006753    39.87  sec\n",
      " 45     0.000050      0.000083      0.008785      0.009374      0.008759    39.75  sec\n",
      " 46     0.000074      0.000153      0.012274      0.013211      0.012232    39.86  sec\n",
      " 47     0.000064      0.000070      0.008004      0.008661      0.007975    40.17  sec\n",
      " 48     0.000054      0.000037      0.005461      0.006715      0.005405    39.60  sec\n",
      " 49     0.000039      0.000074      0.008275      0.009337      0.008228    39.61  sec\n",
      " 50     0.000079      0.000045      0.006151      0.007232      0.006104    39.52  sec\n",
      " 51     0.000074      0.000033      0.004832      0.005595      0.004798    39.48  sec\n",
      " 52     0.000048      0.000058      0.007146      0.007789      0.007117    39.52  sec\n",
      " 53     0.000085      0.000087      0.008939      0.008999      0.008936    39.39  sec\n",
      " 54     0.000050      0.000048      0.006289      0.006902      0.006262    39.32  sec\n",
      " 55     0.000050      0.000075      0.008234      0.008566      0.008220    39.35  sec\n",
      " 56     0.000057      0.000029      0.004397      0.005095      0.004367    39.40  sec\n",
      " 57     0.000056      0.000053      0.006795      0.007507      0.006763    39.39  sec\n",
      " 58     0.000075      0.000038      0.005437      0.006421      0.005394    39.43  sec\n",
      " 59     0.000077      0.000057      0.006831      0.007880      0.006785    39.32  sec\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      " 60     0.000077      0.000058      0.006984      0.008383      0.006922    39.35  sec\n",
      " 61     0.000068      0.000079      0.008412      0.009107      0.008381    39.37  sec\n",
      " 62     0.000064      0.000145      0.011728      0.011909      0.011720    39.33  sec\n",
      " 63     0.000071      0.000071      0.007675      0.010492      0.007550    39.37  sec\n",
      " 64     0.000047      0.000051      0.006527      0.006965      0.006508    39.41  sec\n",
      " 65     0.000055      0.000039      0.005542      0.006514      0.005499    39.37  sec\n",
      " 66     0.000047      0.000105      0.009889      0.010450      0.009864    39.39  sec\n",
      " 67     0.000060      0.000046      0.006236      0.006699      0.006215    39.41  sec\n",
      " 68     0.000047      0.000053      0.006766      0.006734      0.006767    39.32  sec\n",
      " 69     0.000068      0.000035      0.005075      0.005886      0.005039    39.38  sec\n",
      " 70     0.000044      0.000040      0.005539      0.005651      0.005534    39.39  sec\n",
      " 71     0.000058      0.000141      0.011711      0.011660      0.011713    39.35  sec\n",
      " 72     0.000103      0.000043      0.005775      0.005941      0.005768    39.35  sec\n",
      " 73     0.000045      0.000050      0.006573      0.007414      0.006536    39.43  sec\n",
      " 74     0.000061      0.000066      0.007738      0.008633      0.007698    39.37  sec\n",
      " 75     0.000062      0.000032      0.004915      0.005516      0.004888    39.36  sec\n",
      " 76     0.000047      0.000083      0.008910      0.010100      0.008857    39.35  sec\n",
      " 77     0.000059      0.000033      0.005052      0.005805      0.005019    39.52  sec\n",
      " 78     0.000068      0.000086      0.008896      0.010429      0.008828    39.39  sec\n",
      " 79     0.000075      0.000044      0.005839      0.006222      0.005822    39.38  sec\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      " 80     0.000051      0.000049      0.006097      0.005863      0.006107    39.33  sec\n",
      " 81     0.000076      0.000037      0.005270      0.006135      0.005232    39.39  sec\n",
      " 82     0.000046      0.000033      0.005018      0.005368      0.005003    39.47  sec\n",
      " 83     0.000055      0.000095      0.009332      0.009316      0.009332    39.36  sec\n",
      " 84     0.000123      0.000042      0.005789      0.006420      0.005761    39.30  sec\n",
      " 85     0.000058      0.000033      0.004705      0.005246      0.004682    39.35  sec\n",
      " 86     0.000087      0.000047      0.005894      0.007035      0.005843    39.36  sec\n",
      " 87     0.000080      0.000116      0.010175      0.011366      0.010123    39.29  sec\n",
      " 88     0.000058      0.000078      0.008425      0.009649      0.008371    39.42  sec\n",
      " 89     0.000053      0.000037      0.005086      0.006470      0.005026    39.35  sec\n",
      " 90     0.000056      0.000042      0.005852      0.006340      0.005830    39.30  sec\n",
      " 91     0.000076      0.000040      0.005604      0.006639      0.005559    39.38  sec\n",
      " 92     0.000055      0.000035      0.005207      0.005982      0.005173    39.32  sec\n",
      " 93     0.000049      0.000121      0.010941      0.010242      0.010972    39.57  sec\n",
      " 94     0.000066      0.000038      0.005355      0.006097      0.005322    39.34  sec\n",
      " 95     0.000055      0.000052      0.006647      0.006281      0.006663    39.43  sec\n",
      " 96     0.000058      0.000043      0.005652      0.006285      0.005624    39.48  sec\n",
      " 97     0.000050      0.000045      0.006230      0.007035      0.006195    39.33  sec\n",
      " 98     0.000060      0.000034      0.004932      0.005236      0.004919    39.29  sec\n",
      " 99     0.000060      0.000040      0.005522      0.005773      0.005511    39.56  sec\n",
      "epoch   trn_loss      val_loss    val_rel_err  val_rel_err_o val_rel_err_h run_time\n",
      " 100    0.000045      0.000034      0.005074      0.005381      0.005060    39.45  sec\n",
      "\n",
      "Saving training results to './studies/logs/l-town-chebnet-predictor-pipe_length-minmax-self_loop--31.csv'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"+------------------+\")\n",
    "print(\"| Training session |\")\n",
    "print(\"| Timesteps: {}    |\".format(timesteps))\n",
    "print(\"+------------------+\\n\")\n",
    "\n",
    "print(\"Training starting...\\n\")\n",
    "\n",
    "# Train for the predefined number of epochs\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Start a stopwatch timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train a single epoch, passing the optimizer and current epoch number\n",
    "    model.train_one_epoch(optimizer = optimizer)\n",
    "    \n",
    "    # Validate the model after the gradient update\n",
    "    model.validate()\n",
    "    \n",
    "    # Update the model results for the current epoch\n",
    "    model.update_results()\n",
    "    \n",
    "    # Print stats for the epoch and the execution time\n",
    "    model.print_stats(time.time() - start_time)\n",
    "    \n",
    "    # If this is the best model\n",
    "    if model.val_loss < model.best_val_loss:\n",
    "        # We save it\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # If model is not improving\n",
    "    if estop.step(torch.tensor(model.val_loss)):\n",
    "        print('Early stopping activated...')\n",
    "        break\n",
    "\n",
    "print(\"\\nSaving training results to '{}'...\\n\".format(log_path))\n",
    "model.results.to_csv(log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
